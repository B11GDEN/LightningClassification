#transformer
model_name:
  value: vit_tiny_patch16_224

# attention parameter
dim:
  desc: q, k, v dimensions
  value: 192
num_heads:
  desc: number of heads
  value: 3
q_kernel:
  desc: query kernel type
  value: l2
k_kernel:
  desc: key kernel type
  value: l2
qkv_bias:
  value: True
kv_drop:
  value: 0.
proj_drop:
  value: 0.

# dataset
dataset_name:
  desc: Dataset name (cifar100, imagenet, ...)
  value: cifar10
num_workers:
  desc: Number of workers
  value: 24
batch_size:
  desc: Size of each mini-batch
  value: 4096

# trainer
epochs:
  desc: Number of epochs to train over
  value: 100
accelerator:
  desc: gpu, cpu
  value: gpu
gpus:
  value: None
strategy:
  value: None